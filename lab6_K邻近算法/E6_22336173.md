# **ä¸­å±±å¤§å­¦è®¡ç®—æœºå­¦é™¢** **æœ¬ç§‘ç”Ÿå®éªŒæŠ¥å‘Š**ï¼ˆ2023å­¦å¹´æ˜¥å­£å­¦æœŸï¼‰

## è¯¾ç¨‹åç§°ï¼šArtificial Intelligence **äººå·¥æ™ºèƒ½**

| æ•™å­¦ç­çº§        |      | ä¸“ä¸šï¼ˆæ–¹å‘ï¼‰ |      |
| --------------- | ---- | ------------ | ---- |
| å­¦å·  2233 6173 |      | å§“å  ç½—å¼˜æ° |      |

## å®éªŒé¢˜ç›® KNNä¸æœ´ç´ è´å¶æ–¯å¤šåˆ†ç±»å™¨

![image-20240509100355183](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240509100355183.png)

## å®éªŒå†…å®¹

ç®—æ³•åŸç†

KNNç®—æ³•åŸç†ï¼š

1. **åŸºæœ¬æ€æƒ³**ï¼šKNNç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**æ‰¾åˆ°ä¸æ–°æ•°æ®ç‚¹æœ€ç›¸ä¼¼çš„Kä¸ªè®­ç»ƒæ•°æ®ç‚¹**æ¥è¿›è¡Œåˆ†ç±»ã€‚å®ƒåŸºäºç‰¹å¾ç©ºé—´ä¸­çš„é‚»è¿‘æ€§ï¼Œå³å‡è®¾ç›¸ä¼¼çš„æ•°æ®ç‚¹å…·æœ‰ç›¸ä¼¼çš„æ ‡ç­¾ã€‚
2. **è·ç¦»åº¦é‡**ï¼šKNNç®—æ³•ä½¿ç”¨è·ç¦»åº¦é‡æ¥ç¡®å®šæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚å¸¸ç”¨çš„è·ç¦»åº¦é‡åŒ…æ‹¬æ¬§æ°è·ç¦»ã€æ›¼å“ˆé¡¿è·ç¦»ã€é—µå¯å¤«æ–¯åŸºè·ç¦»ç­‰ã€‚
3. **åˆ†ç±»è¿‡ç¨‹**ï¼š
4. - å¯¹äºç»™å®šçš„æ–°æ•°æ®ç‚¹ï¼Œè®¡ç®—å®ƒä¸è®­ç»ƒé›†ä¸­æ¯ä¸ªæ•°æ®ç‚¹çš„è·ç¦»ã€‚
   - æ ¹æ®è·ç¦»æ‰¾åˆ°ä¸æ–°æ•°æ®ç‚¹æœ€è¿‘çš„Kä¸ªè®­ç»ƒæ•°æ®ç‚¹ã€‚
   - æ ¹æ®è¿™Kä¸ªé‚»å±…çš„æ ‡ç­¾ï¼Œé‡‡ç”¨å¤šæ•°æŠ•ç¥¨çš„æ–¹å¼ç¡®å®šæ–°æ•°æ®ç‚¹çš„ç±»åˆ«ã€‚
5. **Kå€¼é€‰æ‹©**ï¼šKå€¼æ˜¯ç®—æ³•çš„ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œå®ƒå†³å®šäº†æ¨¡å‹çš„å¤æ‚åº¦å’Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚é€šå¸¸é€šè¿‡äº¤å‰éªŒè¯ç­‰æ–¹æ³•æ¥é€‰æ‹©æœ€ä¼˜çš„Kå€¼ã€‚

æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•åŸç†ï¼š

1. **è´å¶æ–¯å®šç†**ï¼šæœ´ç´ è´å¶æ–¯ç®—æ³•åŸºäºè´å¶æ–¯å®šç†ï¼Œå³æ ¹æ®æ¡ä»¶æ¦‚ç‡æ¥ä¼°è®¡ç»™å®šä¸€ä¸ªç±»åˆ«çš„æƒ…å†µä¸‹ï¼Œå¦ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚è´å¶æ–¯å®šç†è¡¨è¾¾å¦‚ä¸‹ï¼š

2. ğ‘ƒ(ğ´âˆ£ğµ)=ğ‘ƒ(ğµâˆ£ğ´)â‹…ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)*P*(*A*âˆ£*B*)=*P*(*B*)*P*(*B*âˆ£*A*)â‹…*P*(*A*)

3. å…¶ä¸­ï¼Œğ‘ƒ(ğ´âˆ£ğµ)*P*(*A*âˆ£*B*) æ˜¯åœ¨ç»™å®šBçš„æƒ…å†µä¸‹Açš„æ¦‚ç‡ï¼Œğ‘ƒ(ğµâˆ£ğ´)*P*(*B*âˆ£*A*) æ˜¯åœ¨ç»™å®šAçš„æƒ…å†µä¸‹Bçš„æ¦‚ç‡ï¼Œğ‘ƒ(ğ´)*P*(*A*) å’Œ ğ‘ƒ(ğµ)*P*(*B*) åˆ†åˆ«æ˜¯Aå’ŒBçš„è¾¹ç¼˜æ¦‚ç‡ã€‚

4. **ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼šæœ´ç´ è´å¶æ–¯ç®—æ³•å‡è®¾ç»™å®šç±»åˆ«çš„æƒ…å†µä¸‹ï¼Œç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚è¿™æ„å‘³ç€ç‰¹å¾ä¹‹é—´çš„å­˜åœ¨ä¸ä¼šå½±å“å½¼æ­¤çš„å‡ºç°æ¦‚ç‡ã€‚å°½ç®¡è¿™ä¸ªå‡è®¾åœ¨ç°å®ä¸­å¹¶ä¸æ€»æ˜¯æˆç«‹ï¼Œä½†å®ƒç®€åŒ–äº†è®¡ç®—ï¼Œä½¿å¾—æœ´ç´ è´å¶æ–¯ç®—æ³•æ˜“äºå®ç°ä¸”é«˜æ•ˆã€‚

5. **æ¨¡å‹è®­ç»ƒ**ï¼š

6. - è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ ğ‘ƒ(ğ¶ğ‘˜)*P*(*C**k*)ï¼Œå³åœ¨ä¸è€ƒè™‘ä»»ä½•ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ ·æœ¬å±äºç±»åˆ« ğ¶ğ‘˜*C**k* çš„æ¦‚ç‡ã€‚
   - å¯¹äºæ¯ä¸ªç‰¹å¾ ğ‘¥ğ‘–*x**i*ï¼Œè®¡ç®—åœ¨ç»™å®šç±»åˆ« ğ¶ğ‘˜*C**k* ä¸‹çš„æ¡ä»¶æ¦‚ç‡ ğ‘ƒ(ğ‘¥ğ‘–âˆ£ğ¶ğ‘˜)*P*(*x**i*âˆ£*C**k*)ï¼Œå³ç‰¹å¾ ğ‘¥ğ‘–*x**i* å‡ºç°åœ¨å±äºç±»åˆ« ğ¶ğ‘˜*C**k* çš„æ ·æœ¬ä¸­çš„æ¦‚ç‡ã€‚

7. **æ¨¡å‹é¢„æµ‹**ï¼š

   â€‹	å¯¹äºä¸€ä¸ªæ–°æ ·æœ¬ï¼Œè®¡ç®—å…¶å±äºæ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡ã€‚

   â€‹	æ ¹æ®åéªŒæ¦‚ç‡é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœã€‚

ä¼ªä»£ç ï¼š

```
#TF-IDF
1. å¯¹æ–‡æ¡£é›†åˆ D è¿›è¡Œé¢„å¤„ç†ï¼š
   - åˆ†è¯
   - å»é™¤åœç”¨è¯
   - è½¬æ¢ä¸ºå°å†™

2. è®¡ç®—æ¯ä¸ªè¯çš„TF-IDFå€¼ï¼š
   2.1 è®¡ç®—è¯é¢‘TFï¼ˆTerm Frequencyï¼‰ï¼š
       - å¯¹äºæ¯ä¸ªæ–‡æ¡£ d âˆˆ Dï¼š
           - ç»Ÿè®¡è¯é¢‘ TF(w, d) = (å•è¯ w åœ¨æ–‡æ¡£ d ä¸­å‡ºç°çš„æ¬¡æ•°) / (æ–‡æ¡£ d çš„æ€»è¯æ•°)
   2.2 è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡IDFï¼ˆInverse Document Frequencyï¼‰ï¼š
       - å¯¹äºæ¯ä¸ªå•è¯ wï¼š
           - ç»Ÿè®¡åŒ…å«è¯¥å•è¯çš„æ–‡æ¡£æ•°é‡ DF(w)
           - è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡ IDF(w) = log((æ€»æ–‡æ¡£æ•°) / (DF(w) + 1)) + 1
   2.3 è®¡ç®—TF-IDFå€¼ï¼š
       - å¯¹äºæ¯ä¸ªæ–‡æ¡£ d âˆˆ Dï¼Œæ¯ä¸ªå•è¯ wï¼š
           - TF-IDF(w, d) = TF(w, d) * IDF(w)

3. æ„å»ºTF-IDFç¼–ç çŸ©é˜µï¼š
   - å¯¹äºæ¯ä¸ªæ–‡æ¡£ d âˆˆ Dï¼š
       - å°† TF-IDF(w, d) æ·»åŠ åˆ° TF-IDF ç¼–ç çŸ©é˜µçš„ç›¸åº”ä½ç½®
   - è¿”å› TF-IDF ç¼–ç çŸ©é˜µ
```



```
#KNN 
function KNN_predict(new_sample, dataset, labels, k):
    distances = [] # ç”¨äºå­˜å‚¨æ–°æ ·æœ¬ä¸æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„è·ç¦»
    for i in range(len(dataset)):
        # è®¡ç®—æ–°æ ·æœ¬ä¸æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„è·ç¦»
        distance = calculate_distance(new_sample, dataset[i])
        distances.append((distance, labels[i])) # å°†è·ç¦»å’Œæ ‡ç­¾ç»„æˆå…ƒç»„å­˜å…¥åˆ—è¡¨

    # å¯¹è·ç¦»è¿›è¡Œæ’åº
    distances.sort(key=lambda x: x[0])
    
    # ç»Ÿè®¡æœ€è¿‘çš„kä¸ªæ ·æœ¬ä¸­å„ç±»åˆ«çš„å‡ºç°æ¬¡æ•°
    class_votes = {}
    for i in range(k):
        label = distances[i][1]
        class_votes[label] = class_votes.get(label, 0) + 1
    
    # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœ
    predicted_label = max(class_votes, key=class_votes.get)
    return predicted_label
```

æµç¨‹å›¾ï¼š

æ•°æ®é¢„ç¼–ç ï¼šæœ¬å®éªŒä½¿ç”¨äº†ä¸¤ç§ç¼–ç æ–¹æ³•ï¼šOne-Hotå’ŒTF-IDFï¼Œåœ¨å®éªŒç»“æœä¼šå‘ˆç°ä¸¤ç§çš„æ•ˆæœã€‚

```mermaid
graph TD;
	A[è®­ç»ƒé›†æ•°æ®]-->B[ç¬¬ä¸€æ¬¡å¾ªç¯è®­ç»ƒé›†ï¼Œè®°å½•å‡ºç°çš„è¯æ„æˆçš„è¯è¡¨ï¼›å°†è¯è¡¨ä½œä¸ºå‘é‡åŸºï¼Œ
	]-->C[ç¬¬äºŒæ¬¡å¾ªç¯è®­ç»ƒé›†ï¼Œå¯¹åº”è¯å‘é‡åŸºï¼Œæ¯ä¸€ä¸ªå¥å­å‡ºç°çš„è¯åœ¨è¯¥ç‰¹å¾ç»´ç½®ä¸º1ï¼Œ
	æ²¡æœ‰å‡ºç°è¿‡çš„ç½®ä¸º0ï¼Œæ­¤ç§°ä¹‹ä¸ºOne-Hotç¼–ç ]-->D[è®¡ç®—IDF:logæ–‡æ¡£æ€»æ•°/å«æœ‰è¯¥è¯çš„æ–‡æ¡£æ•°+1]-->E[è®¡ç®—TFï¼šè¯¥è¯åœ¨è¯¥å¥å­å‡ºç°çš„é¢‘ç‡]-->F[TF*IDFè·å¾—TF-IDFç¼–ç ]
	G[æµ‹è¯•é›†æ•°æ®]-->B[ç¬¬ä¸€æ¬¡å¾ªç¯è®­ç»ƒé›†ï¼Œè®°å½•å‡ºç°çš„è¯æ„æˆçš„è¯è¡¨ï¼›å°†è¯è¡¨ä½œä¸ºå‘é‡åŸºï¼Œ
	]-->C[ç¬¬äºŒæ¬¡å¾ªç¯è®­ç»ƒé›†ï¼Œå¯¹åº”è¯å‘é‡åŸºï¼Œæ¯ä¸€ä¸ªå¥å­å‡ºç°çš„è¯åœ¨è¯¥ç‰¹å¾ç»´ç½®ä¸º1ï¼Œ
	æ²¡æœ‰å‡ºç°è¿‡çš„ç½®ä¸º0ï¼Œæ­¤ç§°ä¹‹ä¸ºOne-Hotç¼–ç ]-->H[è®¡ç®—IDF:ä½¿ç”¨è®­ç»ƒé›†çš„IDFæ›¿ä»£]-->E[è®¡ç®—TFï¼šè¯¥è¯åœ¨è¯¥å¥å­å‡ºç°çš„é¢‘ç‡]-->F[TF*IDFè·å¾—TF-IDFç¼–ç ]
```



```mermaid
graph TD;
	A{Kä»1åˆ°50}
	A-->|K<50|C[ä»£å…¥KNNåˆ†ç±»å™¨è¿ç®—é¢„æµ‹å‡†ç¡®ç‡,
	è®°å½•æœ€å¤§å‡†ç¡®ç‡çš„MaxK]-->E[å°†æµ‹è¯•é›†æ¯ä¸€ä¸ªæµ‹è¯•æ•°æ®ä¸è®­ç»ƒé›†è®¡ç®—è·ç¦»ï¼Œ
	ä½¿ç”¨äºŒèŒƒæ•°å®šä¹‰è·ç¦»è®¡ç®—å…¬å¼ï¼Œ
	å–å‡ºè·ç¦»æœ€å°çš„kä¸ªæµ‹è¯•é›†ç‚¹ï¼Œ
	å°†å…¶ç±»åˆ«æ€»æ•°ä½œä¸ºé¢„æµ‹åˆ†ç±»]-->F[è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡ï¼Œä¿å­˜æœ€å¤§é¢„æµ‹å‡†ç¡®ç‡
	]-->A
	A-->|k=50|D[å°†æœ€å¤§å‡†ç¡®ç‡MaxKå¾—åˆ°çš„é¢„æµ‹ç»“æœä¿å­˜]
```



å…³é”®ä»£ç å±•ç¤ºï¼ˆå¸¦æ³¨é‡Šï¼‰

```python
'''
	è®­ç»ƒé›†çš„ç¼–ç æ–¹æ³•ï¼Œéœ€è¦å­˜å‚¨IDFç»™æµ‹è¯•é›†ä½¿ç”¨
'''
#ä½¿ç”¨TF-IDFåŠ æƒç¼–ç 
#è®¡ç®—IDF
IDF = np.zeros(len(df.columns)-2) #åˆå§‹åŒ–IDF
for word in df.columns[2:]:
    sum = 0 #è®¡ç®—å‡ºç°å•è¯çš„æ–‡æ¡£æ•°
    condition = df[word] == 1 #å‡ºç°è¿‡çš„æ¡ä»¶
    sum = sum + len(df[condition]) #è®¡ç®—å‡ºç°å•è¯çš„æ–‡æ¡£æ•°
    IDF[df.columns.get_loc(word)-2] = math.log(len(df)/sum+1) #è®¡ç®—IDF
#ä¿å­˜IDF
df_IDF = pd.DataFrame(IDF, index=df.columns[2:], columns=["IDF"])
df_IDF.to_csv("IDF.csv")

#è®¡ç®—TF
for i in range(len(df)):
    words = df.iloc[i]["emotion_words"].split()
    for word in words:
        df.at[df.index[i], word] = counter(words, word)/len(words) #è®¡ç®—é¢‘ç‡
        df.at[df.index[i], word] = df.at[df.index[i], word]*IDF[df.columns.get_loc(word)-2]         #è®¡ç®—TF-IDF
df.to_csv("train_tfidf.csv") #ä¿å­˜TF-IDFç¼–ç çš„æ•°æ®
```



```python
'''
	KNNåˆ†ç±»å™¨ï¼Œä½¿ç”¨äºŒç»´è·ç¦»æ¥ä½œä¸ºåº¦é‡ï¼Œè®¡ç®—Kä¸ªæœ€è¿‘çš„ç‚¹ï¼Œå–åˆ†ç±»æ€»æ•°ä½œä¸ºç±»åˆ«é¢„æµ‹
'''
#KNNåˆ†ç±»å™¨
class K_classifier:
    def __init__(self, k, train_data, train_label):
        self.k = k
        '''
        train_data shape : (n_samples, n_features)
        '''
        self.train_data = train_data
        '''
        train_label shape : (n_samples, 1)
        '''
        self.train_label = train_label

    def __call__(self, x):
        '''
            this function is to predict the label of x
        '''
        distances = []
        for i in range(self.train_data.shape[0]):
            distance = Minkowski_distance(x, self.train_data[i], 2)  #2ç»´è·ç¦»
            distances.append((distance, self.train_label[i], i))
        distances.sort(key=lambda x:x[0]) #æŒ‰ç…§è·ç¦»æ’åº,ä»å°åˆ°å¤§
        labels = [item[1] for item in distances[:self.k]] #å–å‰kä¸ªçš„label
        index = [item[2] for item in distances[:self.k]]
        return max(set(labels), key=labels.count) #è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„label
```

```python
'''
	ä¸»å‡½æ•°ä¸­çš„è°ƒç”¨æ–¹æ³•ï¼Œç”±äºéœ€è¦è®­ç»ƒè¶…å‚æ•°K,æ‰€ä»¥éœ€è¦éå†Kå¯èƒ½çš„èŒƒå›´ï¼Œä»¥è·å¾—åˆ†ç±»æ•ˆæœæœ€å¥½çš„k
'''

Max_k = 0
Max_accuracy = 0
import matplotlib.pyplot as plt
x = []
y = []
for k in range(1, 50):
    K_classifier = K_classify.K_classifier(k, train_data, train_label) #åˆ›å»ºK_classifierå¯¹è±¡
    predict_label = []
    for i in range(len(test_data)):
        predict_label.append(K_classifier(test_data[i]))

    df_predict_label = pd.DataFrame(predict_label, columns=["emotionId"])

    #è®¡ç®—å‡†ç¡®ç‡
    #å…ˆè·å¾—æ­£ç¡®çš„æ ‡ç­¾
    correct_label = pd.read_csv("test_label.csv") #è¯»å–æ­£ç¡®çš„æ ‡ç­¾
    #è®¡ç®—å‡†ç¡®ç‡
    correct_label = correct_label["emotionId"].values
    count = 0
    for i in range(len(correct_label)):
        if correct_label[i] == predict_label[i]:
            count += 1
    accuracy = count / len(correct_label)
    if accuracy > Max_accuracy:
        Max_accuracy = accuracy
        Max_k = k
    x.append(k)
    y.append(accuracy)

```

åˆ›æ–°ç‚¹&ä¼˜åŒ–ï¼ˆå¦‚æœæœ‰ï¼‰

1. é€šè¿‡è®­ç»ƒè·å¾—æœ€ä½³è¶…å‚æ•°ã€‚
2. é€šè¿‡ä¸¤ç§ç¼–ç æ ¼å¼æ¥æ¯”è¾ƒæ•ˆæœã€‚

## å®éªŒç»“æœåŠåˆ†æ

1\. å®éªŒç»“æœå±•ç¤ºç¤ºä¾‹ï¼ˆå¯å›¾å¯è¡¨å¯æ–‡å­—ï¼Œå°½é‡å¯è§†åŒ–ï¼‰

â€‹	è¾“å‡ºä¸ºtest.output.csv:

![image-20240509105740351](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240509105740351.png)

2\. è¯„æµ‹æŒ‡æ ‡å±•ç¤ºåŠåˆ†æï¼ˆæœºå™¨å­¦ä¹ å®éªŒå¿…é¡»æœ‰æ­¤é¡¹ï¼Œå…¶å®ƒå¯åˆ†æè¿è¡Œæ—¶é—´ç­‰ï¼‰

ä½¿ç”¨TF-IDFçš„åˆ†ç±»æ•ˆæœï¼šæœ€å¥½çš„Kæ˜¯15ï¼Œåˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°äº†36.8%

![test_tfidf](C:\Users\rogers\Documents\learning in cs\ai\E6_22336173_Ké‚»è¿‘ç®—æ³•\result\test_tfidf.png)

ä½¿ç”¨One-Hotç¼–ç å®éªŒå¾—åˆ°çš„ç»“æœï¼šæœ€å¥½çš„Kæ˜¯21ï¼Œåˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°38.1%

![test_onehot](C:\Users\rogers\Documents\learning in cs\ai\E6_22336173_Ké‚»è¿‘ç®—æ³•\result\test_onehot.png)

## å‚è€ƒèµ„æ–™

PSï¼šå¯ä»¥è‡ªå·±è®¾è®¡æŠ¥å‘Šæ¨¡æ¿ï¼Œä½†æ˜¯å†…å®¹å¿…é¡»åŒ…æ‹¬ä¸Šè¿°çš„å‡ ä¸ªéƒ¨åˆ†ï¼Œä¸éœ€è¦å†™å®éªŒæ„Ÿæƒ³
