# **中山大学计算机学院** **本科生实验报告**（2023学年春季学期）

## 课程名称：Artificial Intelligence **人工智能**

| 教学班级        |      | 专业（方向） |      |
| --------------- | ---- | ------------ | ---- |
| 学号  2233 6173 |      | 姓名  罗弘杰 |      |

## 实验题目

![image-20240518170351180](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240518170351180.png)

## 实验内容

- 算法原理

  ​	卷积神经网络是神经网络的一种，该算法用于识别数据中的模块，神经网络总体上村咋iyu每一层的神经元中，伴随他们的学习参数和偏置，CNN可分为不同的模块：

  1.**输入层(INPUT)**：用于数据的输入。
  当输入一张32*32的jpg图片的时候，输入层能读取到32*32*3的矩阵，3是它的深度（即R、G、B）。

  2.卷积层，激活层，池化层

  **卷积层**(CONV)：使用卷积核进行特征提取和特征映射。
  机器学习识别图片的过程中，并不是一下子整张图同时识别，而是对于图片中的每一个特征首先局部感知，然后更高层次对局部进行综合操作，从而得到全局信息。
  **激励层**：由于卷积也是一种线性运算，因此需要增加非线性映射。
  所谓激励，实际上是对卷积层的输出结果做一次非线性映射。
  　　如果不用激励函数（其实就相当于激励函数是f(x)=x），这种情况下，每一层的输出都是上一层输入的线性
  函数。容易得出，无论有多少神经网络层，输出都是输入的线性组合，与没有隐层的效果是一样的，这就是最原
  始的感知机了。
  常用的激励函数有：

  Sigmoid
  Tanh
  ReLU
  Leaky ReLU
  ELU
  Maxout
  激励函数建议：首先ReLU，因为迭代速度快，但是有可能效果不加。如果ReLU失效的情况下，考虑使用
  Leaky ReLU或者Maxout，此时一般情况都可以解决。Tanh函数在文本和音频处理有比较好的效果。
  池化层(POOL)：进行下采样，对特征图稀疏处理，减少数据运算量。
  池化，也称为欠采样或下采样。要用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。

  **池化层**

  为了减小特征图的尺寸并提取更加抽象的特征，卷积层通常与池化层（Pooling Layer）结合使用。池化层通过对特征图进行降采样操作，减少计算量并增强特征的平移不变性。

  **全连接层(FC)**：也称它为输出层，也称它为输出层，用于输出卷积计算后的结果。
  经过前面若干次卷积+激励+池化后，终于来到了输出层，模型会将学到的一个高质量的特征图片全连接层。其实在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入dropout操作，来随机删除神经网络中的部分神经元，来解决此问题。还可以进行局部归一化（LRN）、数据增强等操作，来增加鲁棒性。

- 流程图

  ![卷积神经网络（CNN）原理及应用 - 知乎](https://pic3.zhimg.com/v2-2140eb4a0e0fe56c38add63f1ba3e962_r.jpg)

- 关键代码展示（带注释）

  使用pytorch框架

  ```python
  
  class SimpleCNN(nn.Module):#tensor shape: (batch_size, channels, H, W)
      def __init__(self):
          super(SimpleCNN, self).__init__()
          self.conv1 = nn.Sequential( 
              nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2), # 32 16 224 224
              nn.ReLU(),
              nn.MaxPool2d(kernel_size=4)
          ) #32 16 56 56 
  
          self.conv2 = nn.Sequential(
              nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2), #32 32 56 56
              nn.ReLU(),
              nn.MaxPool2d(kernel_size=4)
          ) #32 32 14 14
  
          self.conv3 = nn.Sequential(
              nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2), # 32 64 7 7weis
              nn.ReLU(),
              nn.MaxPool2d(2)
          ) #32 64 7 7
  
          self.out = nn.Linear(64 * 7 * 7, 5)  # Adjusted to 5 classes
  
      def forward(self, x):# input x:3 *224 *224
          x = self.conv1(x)
          x = self.conv2(x)
          x = self.conv3(x)
          x = x.view(x.size(0), -1)# Flatten the tensor， output batch*(64*7*7)
          x = self.out(x) # output shape: (batch_size, 5)
          return x
  
  ```

  ​	网络框架简介：使用了3个卷积层（包含Relu激活和Max池化层），1个全连接层

  第一个卷积模块：

  - **输入通道数**：3（假设输入图像是RGB图像）。
  - **输出通道数**：16。
  - **卷积核大小**：5x5。
  - **步幅**：1。
  - **填充**：2（使得卷积操作后输出的空间尺寸不变）。(输出尺寸=⌊步幅输入尺寸+2×填充−卷积核尺寸⌋+1)
  - **激活函数**：ReLU。
  - **最大池化**：4x4（将特征图尺寸减少到1/4）

  第二个卷积模块：

  - **输入通道数**：16（来自前一层的输出）。
  - **输出通道数**：32。
  - **卷积核大小**：5x5。
  - **步幅**：1。
  - **填充**：2。
  - **激活函数**：ReLU。
  - **最大池化**：4x4

  第三个卷积模块：

  - **输入通道数**：32（来自前一层的输出）。
  - **输出通道数**：64。
  - **卷积核大小**：5x5。
  - **步幅**：1。
  - **填充**：2。
  - **激活函数**：ReLU。
  - **最大池化**：2x2。

  全连接层：

  - **输入特征数**：64 * 7 * 7（由最后一个卷积层的输出展平得到）。
  - **输出特征数**：5（假设有5个类别进行分类）。

  ```python
  for i, data in enumerate(trainloader, 0):
              inputs, labels = data
              inputs, labels = inputs.to(device), labels.to(device) # Move the input and label tensors to the GPU
  
              # Verify the range of labels
              if not torch.all((labels >= 0) & (labels < 5)): 
                  print(f"Label out of bounds detected: {labels}")
                  continue
  
              optimizer.zero_grad()
              outputs = model(inputs)
              loss = criterion(outputs, labels)
              loss.backward() # Backpropagation: compute the gradient of the loss with respect to model parameters
              optimizer.step()
  
              running_loss += loss.item()
              if i % 100 == 99:   # Print every 100 mini-batches
                  print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')
                  running_loss = 0.0
  
          # Append the average loss for the epoch
          train_losses.append(running_loss / len(trainloader))
  
          # Evaluate on test set
          model.eval()
          correct = 0
          total = 0 #
          with torch.no_grad():
              for data in testloader:
                  images, labels = data
                  images, labels = images.to(device), labels.to(device)
                  outputs = model(images)
                  _, predicted = torch.max(outputs.data, 1) #
                  total += labels.size(0)
                  correct += (predicted == labels).sum().item()
          
          accuracy = 100 * correct / total
          test_accuracies.append(accuracy)
  ```

  ​	下面简要介绍使用的优化器和损失函数

  optimizer:   Adam优化器通过**结合动量和自适应学习率调整**的方式，能够在训练深度学习模型时提供高效且稳定的优化效果。它是目前深度学习中广泛使用的优化算法之一，通常能够快速地收敛到较好的局部最优解。（是对普通梯度下降法的学习率自优化）（初始学习率为lr =0.01）

  loss 函数：使用的是交叉熵损失函数

  ###### 数学定义

  假设有一个分类问题，输入数据 𝑥**x** 经过神经网络模型得到预测的类别概率分布 𝑝=(𝑝1,𝑝2,…,𝑝𝐶)**p**=(*p*1,*p*2,…,*p**C*)，其中 𝐶*C* 是类别数目。交叉熵损失函数的计算公式为：
  $$
  corss-entropy-loss=\sum_{i=1}^{C}q_ilog(pi)
  $$
  ​	其中C是类别的数目，qi是该图像的真实类别的one_hot编码， pi是模型给出的属于i类的概率，

  当这个损失函数越小，预测越准确。

  交叉熵损失函数是深度学习中常用的一种损失函数，适用于多类别分类任务。它能够衡量模型预测的概率分布与实际标签之间的差异，通过最小化交叉熵损失来优化神经网络模型，从而提升分类任务的准确性和效率。

## 实验结果及分析

1\. 实验结果展示示例（可图可表可文字，尽量可视化）

```python
if os.path.exists('predictions'):
                    shutil.rmtree('predictions')
                if num_epochs == epoch + 1: # Save the predictions only for the last epoch
                    for j in range(len(images)):
                        img_path = testloader.dataset.samples[j][0]
                        img_name = os.path.basename(img_path)
                        predicted_class = classes[predicted[j]]
                        true_class = classes[labels[j]]
                        save_dir = os.path.join('predictions', predicted_class) # Save the predictions in a folder structure
                        os.makedirs(save_dir, exist_ok=True)
                        # copy the src file to the dst folder
                        # Copy original image to prediction folder
                        shutil.copy(img_path, os.path.join(save_dir, img_name))
                        with open(os.path.join(save_dir, f"{img_name}.txt"), 'w') as f:
                            f.write(f"Predicted class: {predicted_class}\n")
                            f.write(f"Confidence: {torch.softmax(outputs[j], dim=0)[predicted[j]].item():.4f}\n")
                            f.write(f"True class: {true_class}\n")
```

​	使用以上代码保存预测效果和复制预测的图片到每一个预测文件夹

![image-20240524212616058](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240524212616058.png)

​	在txt文件夹中会展示预测的概率和真是类别

```
#from R.jpg.txt
Predicted class: jinyinhua
Confidence: 0.9988
True class: jinyinhua
```

2\. 评测指标展示及分析（机器学习实验必须有此项，其它可分析运行时间等）

​	训练损失率随epoch变化如下

![image-20240524212329976](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240524212329976.png)

***

​	可以看到测试准确率大体上随着epoch提高而提高，最终达到93%左右![image-20240524212501537](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240524212501537.png)

![image-20240524212337651](C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20240524212337651.png)



## 

PS：可以自己设计报告模板，但是内容必须包括上述的几个部分，不需要写实验感想
